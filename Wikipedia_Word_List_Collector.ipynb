{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Wikipedia Word List Collector\n",
        "\n",
        "This code try to extract lemmas from wikipedia dump file and make 26 txt file each representing set of words with a particular first letter"
      ],
      "metadata": {
        "id": "7FwSqHjEL3wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAOKPvuh7RAk",
        "outputId": "0a689416-d8a6-4390-8b16-50ceb1632915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import time\n",
        "import html\n",
        "\n",
        "# Paths\n",
        "xml_file_path = \"/content/drive/MyDrive/tugas-akhir/idwiki-latest-pages-articles.xml\"\n",
        "output_dir = \"/content/drive/MyDrive/tugas-akhir/dicts/tokenized-idwiki-dict\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "tokens_by_letter = defaultdict(set)\n",
        "\n",
        "# Regular expressions\n",
        "url_pattern = re.compile(r'\\b(?:https?://|www\\.|[a-zA-Z0-9.-]+\\.[a-z]{2,})(/[^\\s]*)?\\b')\n",
        "html_attr_pattern = re.compile(r'\\b(?:style|class|id|color|width|height|bgcolor|align)=\"[^\"]*\"')\n",
        "\n",
        "# Clean and tokenize\n",
        "def clean_and_tokenize(text):\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Remove HTML attributes like color=\"blue\", style=\"...\", etc.\n",
        "    text = re.sub(html_attr_pattern, '', text)\n",
        "\n",
        "    # Remove all kinds of links (http, www, google.com, etc.)\n",
        "    text = re.sub(url_pattern, '', text)\n",
        "\n",
        "    # Remove wikitext and HTML tags\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)          # HTML\n",
        "    text = re.sub(r'\\[\\[.*?\\]\\]', ' ', text)      # Wiki links\n",
        "    text = re.sub(r'\\{\\{.*?\\}\\}', ' ', text)      # Wiki templates\n",
        "    text = re.sub(r'\\|.*?\\|', ' ', text)          # Table cells\n",
        "\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove tokens with numbers or non-ASCII characters\n",
        "    tokens = [t for t in tokens if t.isalpha() and all(ord(c) < 128 for c in t)]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Count total lines\n",
        "print(\"Counting total lines...\")\n",
        "with open(xml_file_path, 'r', encoding='utf-8') as f:\n",
        "    total_lines = sum(1 for _ in f)\n",
        "print(f\"Estimated total lines: {total_lines:,}\\n\")\n",
        "\n",
        "# Parse XML\n",
        "print(\"Starting XML parsing and tokenization...\")\n",
        "start_time = time.time()\n",
        "line_count = 0\n",
        "report_every = 50000\n",
        "\n",
        "skip_title = \"Berkas:Brigjen Patar Sahat Panggabean.jpeg\"\n",
        "skip_user = \"Flow talk page manager\"\n",
        "\n",
        "inside_page = False\n",
        "skip_this_page = False\n",
        "current_title = \"\"\n",
        "current_username = \"\"\n",
        "\n",
        "for event, elem in ET.iterparse(xml_file_path, events=('start', 'end')):\n",
        "    tag = elem.tag.split('}')[-1]\n",
        "\n",
        "    if event == 'start' and tag == 'page':\n",
        "        inside_page = True\n",
        "        skip_this_page = False\n",
        "        current_title = \"\"\n",
        "        current_username = \"\"\n",
        "\n",
        "    if event == 'end':\n",
        "        if tag == 'title':\n",
        "            current_title = elem.text or \"\"\n",
        "            if current_title == skip_title:\n",
        "                skip_this_page = True\n",
        "\n",
        "        elif tag == 'username':\n",
        "            current_username = elem.text or \"\"\n",
        "            if current_username == skip_user:\n",
        "                skip_this_page = True\n",
        "\n",
        "        elif tag == 'text' and not skip_this_page:\n",
        "            line_count += 1\n",
        "            tokens = clean_and_tokenize(elem.text or \"\")\n",
        "            for token in tokens:\n",
        "                if token:\n",
        "                    tokens_by_letter[token[0].lower()].add(token)\n",
        "\n",
        "            if line_count % report_every == 0:\n",
        "                pct = (line_count / total_lines) * 100\n",
        "                print(f\"[Tokenizing] Line {line_count:,} / {total_lines:,} ({pct:.2f}%)\")\n",
        "\n",
        "        elif tag == 'page':\n",
        "            inside_page = False\n",
        "            skip_this_page = False\n",
        "            current_title = \"\"\n",
        "            current_username = \"\"\n",
        "            elem.clear()\n",
        "\n",
        "print(f\"\\nTokenizing complete in {time.time() - start_time:.2f} seconds.\\n\")\n",
        "\n",
        "# Write output files\n",
        "print(\"Writing token files...\")\n",
        "for i, letter in enumerate(\"abcdefghijklmnopqrstuvwxyz\"):\n",
        "    file_path = os.path.join(output_dir, f\"{letter}.txt\")\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for token in sorted(tokens_by_letter[letter]):\n",
        "            f.write(token + \"\\n\")\n",
        "    print(f\"[Filing] {letter}.txt ({i+1}/26)\")\n",
        "\n",
        "print(\"\\n✅ Done! All token files saved to:\", output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v9l_kjjNqlL",
        "outputId": "457c9e90-bd21-49ab-a853-48e590e14ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting total lines...\n",
            "Estimated total lines: 95,438,835\n",
            "\n",
            "Starting XML parsing and tokenization...\n",
            "[Tokenizing] Line 50,000 / 95,438,835 (0.05%)\n",
            "[Tokenizing] Line 100,000 / 95,438,835 (0.10%)\n",
            "[Tokenizing] Line 150,000 / 95,438,835 (0.16%)\n",
            "[Tokenizing] Line 200,000 / 95,438,835 (0.21%)\n",
            "[Tokenizing] Line 250,000 / 95,438,835 (0.26%)\n",
            "[Tokenizing] Line 300,000 / 95,438,835 (0.31%)\n",
            "[Tokenizing] Line 350,000 / 95,438,835 (0.37%)\n",
            "[Tokenizing] Line 400,000 / 95,438,835 (0.42%)\n",
            "[Tokenizing] Line 450,000 / 95,438,835 (0.47%)\n",
            "[Tokenizing] Line 500,000 / 95,438,835 (0.52%)\n",
            "[Tokenizing] Line 550,000 / 95,438,835 (0.58%)\n",
            "[Tokenizing] Line 600,000 / 95,438,835 (0.63%)\n",
            "[Tokenizing] Line 650,000 / 95,438,835 (0.68%)\n",
            "[Tokenizing] Line 700,000 / 95,438,835 (0.73%)\n",
            "[Tokenizing] Line 750,000 / 95,438,835 (0.79%)\n",
            "[Tokenizing] Line 800,000 / 95,438,835 (0.84%)\n",
            "[Tokenizing] Line 850,000 / 95,438,835 (0.89%)\n",
            "[Tokenizing] Line 900,000 / 95,438,835 (0.94%)\n",
            "[Tokenizing] Line 950,000 / 95,438,835 (1.00%)\n",
            "[Tokenizing] Line 1,000,000 / 95,438,835 (1.05%)\n",
            "[Tokenizing] Line 1,050,000 / 95,438,835 (1.10%)\n",
            "[Tokenizing] Line 1,100,000 / 95,438,835 (1.15%)\n",
            "[Tokenizing] Line 1,150,000 / 95,438,835 (1.20%)\n",
            "[Tokenizing] Line 1,200,000 / 95,438,835 (1.26%)\n",
            "[Tokenizing] Line 1,250,000 / 95,438,835 (1.31%)\n",
            "[Tokenizing] Line 1,300,000 / 95,438,835 (1.36%)\n",
            "[Tokenizing] Line 1,350,000 / 95,438,835 (1.41%)\n",
            "[Tokenizing] Line 1,400,000 / 95,438,835 (1.47%)\n",
            "[Tokenizing] Line 1,450,000 / 95,438,835 (1.52%)\n",
            "[Tokenizing] Line 1,500,000 / 95,438,835 (1.57%)\n",
            "[Tokenizing] Line 1,550,000 / 95,438,835 (1.62%)\n",
            "[Tokenizing] Line 1,600,000 / 95,438,835 (1.68%)\n",
            "[Tokenizing] Line 1,650,000 / 95,438,835 (1.73%)\n",
            "[Tokenizing] Line 1,700,000 / 95,438,835 (1.78%)\n",
            "[Tokenizing] Line 1,750,000 / 95,438,835 (1.83%)\n",
            "\n",
            "Tokenizing complete in 1083.16 seconds.\n",
            "\n",
            "Writing token files...\n",
            "[Filing] a.txt (1/26)\n",
            "[Filing] b.txt (2/26)\n",
            "[Filing] c.txt (3/26)\n",
            "[Filing] d.txt (4/26)\n",
            "[Filing] e.txt (5/26)\n",
            "[Filing] f.txt (6/26)\n",
            "[Filing] g.txt (7/26)\n",
            "[Filing] h.txt (8/26)\n",
            "[Filing] i.txt (9/26)\n",
            "[Filing] j.txt (10/26)\n",
            "[Filing] k.txt (11/26)\n",
            "[Filing] l.txt (12/26)\n",
            "[Filing] m.txt (13/26)\n",
            "[Filing] n.txt (14/26)\n",
            "[Filing] o.txt (15/26)\n",
            "[Filing] p.txt (16/26)\n",
            "[Filing] q.txt (17/26)\n",
            "[Filing] r.txt (18/26)\n",
            "[Filing] s.txt (19/26)\n",
            "[Filing] t.txt (20/26)\n",
            "[Filing] u.txt (21/26)\n",
            "[Filing] v.txt (22/26)\n",
            "[Filing] w.txt (23/26)\n",
            "[Filing] x.txt (24/26)\n",
            "[Filing] y.txt (25/26)\n",
            "[Filing] z.txt (26/26)\n",
            "\n",
            "✅ Done! All token files saved to: /content/drive/MyDrive/tugas-akhir/dicts/tokenized-idwiki-dict-2\n"
          ]
        }
      ]
    }
  ]
}